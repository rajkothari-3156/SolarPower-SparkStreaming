{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mleap==0.8.1\n",
    "# !pip install pyspark==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eb8e67b8-00f1-4312-ba94-1d634ffe6491",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mleap\n",
    "import mleap.pyspark\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer\n",
    "from pyspark import keyword_only\n",
    "from pyspark.sql.functions import to_timestamp,hour,minute,when,col,current_timestamp,date_format,lit,unix_timestamp,expr,abs,to_date,rank,datediff\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder,StringIndexer,VectorAssembler\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator,TrainValidationSplit\n",
    "from pyspark.ml.regression import LinearRegression,RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "# from pyspark.ml import Pipeline, Transformer\n",
    "# from pyspark.sql import DataFrame\n",
    "# from pyspark import keyword_only\n",
    "# from pyspark.ml import Transformer\n",
    "# from pyspark.ml.param.shared import HasOutputCols, Param, Params\n",
    "# from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "# from pyspark.sql.functions import lit # for the dummy _transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/jar/mleap_spark_assembly.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp ./mleap_spark_assembly.jar /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/jars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import sagemaker_pyspark\n",
    "import botocore.session\n",
    "\n",
    "session = botocore.session.get_session()\n",
    "# credentials = session.get_credentials()\n",
    "\n",
    "conf = (SparkConf().set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.56.57:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1608e385f8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.config(conf = conf).appName(\"test\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5f38fae8-be38-4acf-a96a-e64301a9c3c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdata_s3 = 's3a://solarpowerbackend/rawdata/'\n",
    "processed_s3 = 's3a://solarpowerbackend/processed/'\n",
    "model_s3 = 's3a://solarpowerbackend/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2341568f-48c4-40c4-8978-a7492f09a1ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df = spark.read.parquet(f'{processed_s3}rul_dataset')\n",
    "raw_df = raw_df.withColumn('TIME',date_format(col('DATE_TIME'),'HH:mm:ss'))\n",
    "raw_df = raw_df.filter(col('TIME').between('06:00:00','18:15:00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "66a93bfb-3428-4cb8-bde6-2d4b908a335c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df = raw_df.withColumn('DATE',to_date(col('DATE_TIME')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e28d1d2f-aa0c-46e6-80a7-88bf4f264a5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "only_fault_df = raw_df.select('DATE_TIME','FAULT_FLAG','SOURCE_KEY','DATE').filter(col('Fault_Flag')==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6308a28e-e0f0-4fad-80b9-6be593b48360",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DATE_TIME: timestamp, FAULT_FLAG: int, SOURCE_KEY: string, DATE: date]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(only_fault_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f677a8db-07e5-4e47-800f-f25b1969fd6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rank_window = Window.partitionBy(only_fault_df['SOURCE_KEY'],only_fault_df['DATE']).orderBy(only_fault_df['DATE_TIME'])\n",
    "# fault_rank_on_day = rank().over(rank_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "69112abb-7687-4050-98d3-2d50017de492",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fault_rank_df = only_fault_df.select(col('SOURCE_KEY').alias('FAULT_SOURCE_KEY'),col('DATE_TIME').alias('FAULT_DATE_TIME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cca49772-5931-49ad-923d-d50624fdb835",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rul_oncondition = (fault_rank_df['FAULT_DATE_TIME']>=raw_df['DATE_TIME']) & (fault_rank_df['FAULT_SOURCE_KEY'] == raw_df['SOURCE_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6f9ea89f-3eef-454b-a102-33f9e70440f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rul_df = fault_rank_df.join(raw_df,on = rul_oncondition,how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "efc82229-5922-47c6-ac38-030f825e0dc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rul_df = rul_df.withColumn('RUL',(col('FAULT_DATE_TIME').cast('long') - col('DATE_TIME').cast('long'))/60 - datediff(col('FAULT_DATE_TIME'),col('DATE_TIME'))*47*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2c7430d3-3c64-410a-a8ac-e10c5a956137",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[FAULT_SOURCE_KEY: string, FAULT_DATE_TIME: timestamp, DATE_TIME: timestamp, AMBIENT_TEMPERATURE: double, MODULE_TEMPERATURE: double, IRRADIATION: double, PLANT_ID: int, SOURCE_KEY: string, DC_POWER: double, AC_POWER: double, DAILY_YIELD: double, TOTAL_YIELD: double, Fault_Flag: int, TIME: string, DATE: date, RUL: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(rul_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ea6df919-d4dc-476f-ab3c-8e5853b1140c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rul_df = rul_df.groupby('SOURCE_KEY','DATE_TIME','AMBIENT_TEMPERATURE','MODULE_TEMPERATURE','IRRADIATION','PLANT_ID','AC_POWER','DC_POWER','DAILY_YIELD','TOTAL_YIELD').min('RUL')\n",
    "rul_df = rul_df.withColumnRenamed('min(RUL)','label')\n",
    "\n",
    "# rul_df = rul_df.filter(col('RUL')>=1440)\n",
    "# rul_df = rul_df.withColumnRenamed('RUL','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "97012c12-abb0-4c13-aa81-378d9b819e51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rul_df = rul_df.withColumn('AC_POWER',col('AC_POWER') - col('AC_POWER')/(col('RUL')+1)*2)\n",
    "rul_df = rul_df.select('SOURCE_KEY', 'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION', 'AC_POWER', 'DC_POWER', 'DAILY_YIELD', 'TOTAL_YIELD','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import keyword_only\n",
    "# from pyspark.ml import Transformer\n",
    "# from pyspark.ml.param.shared import HasOutputCols, Param, Params\n",
    "# from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "# from pyspark.sql.functions import lit # for the dummy _transform\n",
    "\n",
    "# class ACDC_Calulator(\n",
    "#     Transformer, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable,\n",
    "# ):\n",
    "#     value = Param(\n",
    "#         Params._dummy(),\n",
    "#         \"value\",\n",
    "#         \"value to fill\",\n",
    "#     )\n",
    "\n",
    "#     @keyword_only\n",
    "#     def __init__(self, outputCols=None, value=0.0):\n",
    "#         super(ACDC_Calulator, self).__init__()\n",
    "#         self._setDefault(value=0.0)\n",
    "#         kwargs = self._input_kwargs\n",
    "#         self._set(**kwargs)\n",
    "\n",
    "#     @keyword_only\n",
    "#     def setParams(self, outputCols=None, value=0.0):\n",
    "#         \"\"\"\n",
    "#         setParams(self, outputCols=None, value=0.0)\n",
    "#         Sets params for this SetValueTransformer.\n",
    "#         \"\"\"\n",
    "#         kwargs = self._input_kwargs\n",
    "#         return self._set(**kwargs)\n",
    "\n",
    "#     def setValue(self, value):\n",
    "#         \"\"\"\n",
    "#         Sets the value of :py:attr:`value`.\n",
    "#         \"\"\"\n",
    "#         return self._set(value=value)\n",
    "\n",
    "#     def getValue(self):\n",
    "#         \"\"\"\n",
    "#         Gets the value of :py:attr:`value` or its default value.\n",
    "#         \"\"\"\n",
    "#         return self.getOrDefault(self.value)\n",
    "\n",
    "#     def _transform(self, dataset):\n",
    "#         dataset = dataset.withColumn('ACDC',col('AC_POWER')/(col('DC_POWER')+1))\n",
    "#         return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "24522346-3201-477d-b6e3-219e1b976ef3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# acdc_calc = ACDC_Calulator(outputCols=[\"a\", \"b\"], value=123.0)\n",
    "str_ind = StringIndexer().setInputCol('SOURCE_KEY').setOutputCol('SOURCE_KEY_NUM')\n",
    "ohe = OneHotEncoder().setInputCol('SOURCE_KEY_NUM').setOutputCol('encoded_Source_Key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "afd6288a-4d84-4825-a6ca-b8cac45cf6f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "independent_features = ['AMBIENT_TEMPERATURE','MODULE_TEMPERATURE','IRRADIATION','AC_POWER','DC_POWER','DAILY_YIELD','encoded_Source_Key']\n",
    "model_input_vector = VectorAssembler().setInputCols(independent_features).setOutputCol('features')\n",
    "\n",
    "# rul_indexed_df = rul_indexed_df.withColumnRenamed('RUL','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "55606f13-8f2f-45a5-a407-11dc989fc602",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor_4a36af26463d1b4e4514"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LinearRegression().setFeaturesCol('features').setLabelCol('label')\n",
    "lr_model.setPredictionCol('LR_prediction')\n",
    "rf_model = RandomForestRegressor().setFeaturesCol('features').setLabelCol('label')\n",
    "rf_model.setPredictionCol('RF_prediction')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acdc_calc = ACDC_Calulator()\n",
    "rf_pipeline = Pipeline(stages = [str_ind,ohe,model_input_vector,rf_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rf_pipeline.fit(rul_df)\n",
    "rul_trained_df = rf_model.transform(rul_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bf7f3885-6bde-46aa-8b48-d7137e87f9f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rfcv = CrossValidator(estimator = rf_model,\n",
    "#                       estimatorParamMaps = rfparamGrid,\n",
    "#                       evaluator = rfevaluator,\n",
    "#                       numFolds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f6d6165-7309-4953-8a55-8377ae33f312",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rfcvModel = rfcv.fit(rul_trained_df)\n",
    "# print(rfcvModel)\n",
    "\n",
    "# # Use test set here so we can measure the accuracy of our model on new data\n",
    "# rfpredictions = rfcvModel.transform(rul_trained_df)\n",
    "\n",
    "# # cvModel uses the best model found from the Cross Validation\n",
    "# # Evaluate best model\n",
    "# print('RMSE:', rfevaluator.evaluate(rfpredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "edbce178-3d58-4301-bbf5-72181ce961d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# rul_lr_model = lr_model.fit(rul_trained_df)\n",
    "# rul_trained_df = rul_lr_model.transform(rul_trained_df)\n",
    "# print('Linear Regression Error: ' ,str(rul_lr_model.summary.meanAbsoluteError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rul_trained_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b4e5ac58-be58-4c3a-b0fb-a9c42e14de59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rul_rf_model = rf_model.fit(rul_trained_df)\n",
    "# rul_trained_df = rul_rf_model.transform(rul_trained_df)\n",
    "# rul_rf_model.write().overwrite().save(f'{model_s3}RF_RUL_Model')\n",
    "# print('Randomforest Regression Error: ' ,str(rul_rf_model.stages[-1].summary.meanAbsoluteError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "SimpleSparkSerializer().serializeToBundle(\n",
    "    rf_model, \"jar:file:/tmp/rfmodel.zip\", rul_trained_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"/tmp/rfmodel.zip\") as zf:\n",
    "    zf.extractall(\"/tmp/rfmodel\")\n",
    "\n",
    "import tarfile\n",
    "\n",
    "with tarfile.open(\"/tmp/rfmodel.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"/tmp/rfmodel/bundle.json\", arcname=\"bundle.json\")\n",
    "    tar.add(\"/tmp/rfmodel/root\", arcname=\"root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AMBIENT_TEMPERATURE',\n",
       " 'MODULE_TEMPERATURE',\n",
       " 'IRRADIATION',\n",
       " 'AC_POWER',\n",
       " 'DC_POWER',\n",
       " 'DAILY_YIELD',\n",
       " 'encoded_Source_Key']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "independent_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "for col in independent_features:\n",
    "    if col =='encoded_Source_Key':\n",
    "        col = 'SOURCE_KEY'\n",
    "    feature_list.append(str(rul_df.select(col).collect()[0][col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_string = ','.join(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AMBIENT_TEMPERATURE',\n",
       " 'MODULE_TEMPERATURE',\n",
       " 'IRRADIATION',\n",
       " 'AC_POWER',\n",
       " 'DC_POWER',\n",
       " 'DAILY_YIELD',\n",
       " 'encoded_Source_Key']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "independent_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_string = feature_string[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please replace the bucket name with your bucket name where you want to upload the model\n",
    "s3 = boto3.resource(\"s3\")\n",
    "file_name = os.path.join(\"model/RF_RUL_MODEL\", \"rfmodel.tar.gz\")\n",
    "s3.Bucket(\"solarpowerbackend\").upload_file(\"/tmp/rfmodel.tar.gz\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from mleap.pyspark.spark_support import SimpleSparkSerializer\n",
    "\n",
    "# SimpleSparkSerializer().serializeToBundle(\n",
    "#     rul_rf_model, \"model.zip\", rul_trained_df\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": [{\"name\": \"AMBIENT_TEMPERATURE\", \"type\": \"double\"}, {\"name\": \"MODULE_TEMPERATURE\", \"type\": \"double\"}, {\"name\": \"IRRADIATION\", \"type\": \"double\"}, {\"name\": \"AC_POWER\", \"type\": \"double\"}, {\"name\": \"DC_POWER\", \"type\": \"double\"}, {\"name\": \"DAILY_YIELD\", \"type\": \"double\"}, {\"name\": \"SOURCE_KEY\", \"type\": \"string\"}], \"output\": {\"name\": \"RF_prediction\", \"type\": \"double\"}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "schema = {\n",
    "    \"input\": [\n",
    "        {\"name\": \"AMBIENT_TEMPERATURE\", \"type\": \"double\"},\n",
    "        {\"name\": \"MODULE_TEMPERATURE\", \"type\": \"double\"},\n",
    "        {\"name\": \"IRRADIATION\", \"type\": \"double\"},\n",
    "        \n",
    "        {\"name\": \"AC_POWER\", \"type\": \"double\"},\n",
    "        {\"name\": \"DC_POWER\", \"type\": \"double\"},\n",
    "        \n",
    "        {\"name\": \"DAILY_YIELD\", \"type\": \"double\"},\n",
    "        {\"name\": \"SOURCE_KEY\", \"type\": \"string\"}\n",
    "        \n",
    "    ],\n",
    "    \"output\": {\"name\": \"RF_prediction\", \"type\": \"double\"},\n",
    "}\n",
    "schema_json = json.dumps(schema)\n",
    "print(schema_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sparkml.model import SparkMLModel\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "# S3 location of where you uploaded your trained and serialized SparkML model\n",
    "sparkml_data = \"s3://{}/{}/{}\".format(\n",
    "    \"solarpowerbackend\", \"model/RF_RUL_MODEL\", \"rfmodel.tar.gz\"\n",
    ")\n",
    "model_name = \"sparkml-solarpower-\" + timestamp_prefix\n",
    "sparkml_model = SparkMLModel(\n",
    "    model_data=sparkml_data,\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    name=model_name,\n",
    "    # passing the schema defined above by using an environment\n",
    "    # variable that sagemaker-sparkml-serving understands\n",
    "    env={\"SAGEMAKER_SPARKML_SCHEMA\": schema_json},\n",
    ")\n",
    "\n",
    "\n",
    "endpoint_name = \"sparkml-solarpower-rf\" + timestamp_prefix\n",
    "sparkml_model.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.c4.xlarge\", endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import (\n",
    "    json_serializer,\n",
    "    csv_serializer,\n",
    "    json_deserializer,\n",
    "    RealTimePredictor,\n",
    ")\n",
    "# from sagemaker.content_types import CONTENT_TYPE_CSV\n",
    "\n",
    "payload = feature_string\n",
    "predictor = RealTimePredictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=csv_serializer)\n",
    "print(predictor.predict(feature_string))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "SolarPower-RULPreprocessing",
   "notebookOrigID": 4021020246973347,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
